import { NextResponse } from 'next/server'
import OpenAI from 'openai'

export async function POST(request: Request) {
  console.log('=== Transcription API called ===')
  console.log('Timestamp:', new Date().toISOString())
  
  try {
    // Check if OpenAI API key is configured
    if (!process.env.OPENAI_API_KEY) {
      console.error('‚ùå OpenAI API key not configured')
      return NextResponse.json(
        { error: 'OpenAI API-Schl√ºssel nicht konfiguriert. Bitte OPENAI_API_KEY in Netlify Environment Variables setzen.' },
        { status: 500 }
      )
    }

    console.log('‚úÖ OpenAI API key found')
    console.log('üì• Parsing form data...')
    
    const formData = await request.formData()
    const audioFile = formData.get('audio') as File

    if (!audioFile) {
      console.error('‚ùå No audio file in request')
      return NextResponse.json({ error: 'Keine Audio-Datei erhalten' }, { status: 400 })
    }

    console.log('‚úÖ Audio file received:', {
      name: audioFile.name,
      size: `${(audioFile.size / 1024 / 1024).toFixed(2)} MB`,
      type: audioFile.type
    })

    // Check file size (max 25MB for Whisper)
    if (audioFile.size > 25 * 1024 * 1024) {
      console.error('‚ùå File too large:', audioFile.size)
      return NextResponse.json(
        { error: 'Audio-Datei zu gro√ü (max. 25 MB). Bitte eine k√ºrzere Aufnahme erstellen.' },
        { status: 400 }
      )
    }

    console.log('üîÑ Converting audio file...')
    const audioBuffer = await audioFile.arrayBuffer()
    const audioBlob = new Blob([audioBuffer], { type: 'audio/webm' })
    const audioFileForOpenAI = new File([audioBlob], 'recording.webm', { type: 'audio/webm' })

    console.log('‚úÖ Audio file converted for OpenAI')

    // Initialize OpenAI
    console.log('üîß Initializing OpenAI client...')
    const openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
      timeout: 120000, // 2 minutes timeout
      maxRetries: 2
    })

    // Step 1: Transcribe audio using Whisper
    console.log('üéôÔ∏è Starting Whisper transcription...')
    console.log('‚è≥ This may take 30-120 seconds depending on audio length...')
    
    const transcription = await openai.audio.transcriptions.create({
      file: audioFileForOpenAI,
      model: 'whisper-1',
      language: 'de',
      response_format: 'text'
    })

    console.log('‚úÖ Transcription completed!')
    console.log('üìù Text length:', transcription.length, 'characters')
    console.log('Preview:', transcription.substring(0, 100) + '...')

    // Step 2: Format transcription into structured protocol using GPT-4
    console.log('ü§ñ Formatting protocol with GPT-4o-mini...')
    console.log('‚è≥ This may take 10-30 seconds...')
    
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'system',
          content: `Du bist ein professioneller Meeting-Protokollant. Erstelle aus der folgenden Transkription ein strukturiertes Meeting-Protokoll auf Deutsch.

Das Protokoll soll folgendes Format haben (als JSON):
{
  "title": "Ein passender Titel f√ºr das Meeting",
  "date": "Das heutige Datum im Format DD.MM.YYYY HH:MM",
  "participants": "Liste der Teilnehmer (falls erw√§hnt, sonst 'Nicht spezifiziert')",
  "summary": "Eine pr√§gnante Zusammenfassung des Meetings (2-3 S√§tze)",
  "topics": ["Thema 1", "Thema 2", ...] - Array mit allen besprochenen Hauptthemen,
  "actionItems": ["Aufgabe 1", "Aufgabe 2", ...] - Array mit konkreten Aufgaben und n√§chsten Schritten,
  "transcription": "Die vollst√§ndige, leicht bereinigte Transkription"
}

Wichtig:
- Sei pr√§zise und professionell
- Filtere F√ºllw√∂rter in der Zusammenfassung
- Identifiziere klare Aufgaben und To-Dos
- Behalte wichtige Details bei
- Antworte NUR mit dem JSON-Objekt, ohne zus√§tzlichen Text`
        },
        {
          role: 'user',
          content: `Hier ist die Meeting-Transkription:\n\n${transcription}`
        }
      ],
      temperature: 0.3,
      response_format: { type: "json_object" }
    })

    const protocolText = completion.choices[0]?.message?.content
    if (!protocolText) {
      console.error('‚ùå No response from GPT')
      throw new Error('Keine Antwort von GPT erhalten')
    }

    console.log('‚úÖ GPT response received')
    console.log('üìÑ Protocol preview:', protocolText.substring(0, 200) + '...')

    console.log('üîç Parsing JSON protocol...')
    const protocol = JSON.parse(protocolText)

    // Validate protocol structure
    if (!protocol.title || !protocol.summary || !protocol.topics || !protocol.actionItems) {
      console.error('‚ùå Invalid protocol format:', {
        hasTitle: !!protocol.title,
        hasSummary: !!protocol.summary,
        hasTopics: !!protocol.topics,
        hasActionItems: !!protocol.actionItems
      })
      throw new Error('Ung√ºltiges Protokollformat von GPT erhalten')
    }

    console.log('‚úÖ Protocol validated successfully')
    console.log('üìä Protocol stats:', {
      title: protocol.title,
      topics: protocol.topics.length,
      actionItems: protocol.actionItems.length,
      transcriptionLength: protocol.transcription.length
    })

    console.log('‚úÖ Sending successful response')
    return NextResponse.json({ protocol })

  } catch (error) {
    console.error('‚ùå‚ùå‚ùå ERROR in transcription API ‚ùå‚ùå‚ùå')
    console.error('Error type:', error?.constructor?.name)
    console.error('Error details:', error)
    
    let errorMessage = 'Unbekannter Fehler bei der Verarbeitung'
    let statusCode = 500
    
    if (error instanceof Error) {
      errorMessage = error.message
      console.error('üí• Error message:', errorMessage)
      console.error('üìö Error stack:', error.stack)
      
      // Check for specific OpenAI errors
      if (errorMessage.includes('API key') || errorMessage.includes('Incorrect API key')) {
        errorMessage = 'OpenAI API-Schl√ºssel ung√ºltig oder nicht konfiguriert. Bitte OPENAI_API_KEY in Netlify Environment Variables √ºberpr√ºfen.'
        statusCode = 401
      } else if (errorMessage.includes('quota') || errorMessage.includes('insufficient_quota') || errorMessage.includes('billing')) {
        errorMessage = 'OpenAI API-Guthaben aufgebraucht. Bitte Credits auf platform.openai.com aufladen.'
        statusCode = 402
      } else if (errorMessage.includes('timeout') || errorMessage.includes('timed out')) {
        errorMessage = 'Zeit√ºberschreitung bei der Verarbeitung. Bitte versuchen Sie es mit einer k√ºrzeren Aufnahme oder versuchen Sie es sp√§ter erneut.'
        statusCode = 504
      } else if (errorMessage.includes('rate limit') || errorMessage.includes('Too Many Requests')) {
        errorMessage = 'Zu viele Anfragen. Bitte warten Sie einen Moment und versuchen Sie es erneut.'
        statusCode = 429
      } else if (errorMessage.includes('model') || errorMessage.includes('whisper-1') || errorMessage.includes('gpt-4')) {
        errorMessage = 'OpenAI Modell nicht verf√ºgbar. Bitte versuchen Sie es sp√§ter erneut.'
        statusCode = 503
      } else if (errorMessage.includes('network') || errorMessage.includes('ECONNREFUSED') || errorMessage.includes('fetch failed')) {
        errorMessage = 'Netzwerkfehler. Bitte √ºberpr√ºfen Sie Ihre Internetverbindung und versuchen Sie es erneut.'
        statusCode = 503
      }
    }
    
    console.error('üì§ Sending error response:', errorMessage)
    
    return NextResponse.json(
      { 
        error: errorMessage,
        details: error instanceof Error ? error.message : 'Unbekannt',
        timestamp: new Date().toISOString()
      },
      { status: statusCode }
    )
  }
}

// Increase max duration for audio processing
export const maxDuration = 300 // 5 minutes
export const dynamic = 'force-dynamic'

